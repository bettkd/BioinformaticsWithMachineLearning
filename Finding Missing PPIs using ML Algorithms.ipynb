{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spot-checking Classification Algorithms\n",
    "Discovering which algorithms perform well on our machine learning problem. \n",
    "\n",
    "### Algorithms\n",
    "\n",
    "Linear machine learning algorithms:\n",
    "* Logistic Regression.\n",
    "* Linear Discriminant Analysis.\n",
    "\n",
    "Non-linear machine learning algorithms:\n",
    "* k-Nearest Neighbors.\n",
    "* Naive Bayes.\n",
    "* Classification and Regression Trees.\n",
    "* Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1425, 3)\n"
     ]
    }
   ],
   "source": [
    "#Read dataset\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=5)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "fileName = 'Data/Processed/Numerized_Allergy_and_Asthma.txt'\n",
    "names = ['Prot1', 'Prot2', 'Score']\n",
    "ppiData = read_csv(fileName, delimiter='\\t', names=names)\n",
    "\n",
    "ppiList = ppiData.values\n",
    "\n",
    "X = ppiList[:, 0:2]\n",
    "Y = ppiList[:, 2]\n",
    "\n",
    "print(ppiData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# No need for preprocessing (Scaling, Standardizing, Normalizing)\n",
    "# because the X values are all same units representing proteins\n",
    "\n",
    "\n",
    "# Split K data into train and test sets using KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "seed = 7 # purely random number\n",
    "splits = 38 # Arbitrary number from intutition - sqrt(N), where N=Number of datasets\n",
    "kfold = KFold(n_splits=38, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use two linear machine learning algorithms: logistic regression and linear discriminant analysis.\n",
    "\n",
    "### Logistic Regression\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems. You can construct a logistic regression model using the LogisticRegression class.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.97%\tSigma: 0.41\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classification. It too assumes a Gaussian distribution for the numerical input variables. You can construct an LDA model using the LinearDiscriminantAnalysis class.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html### Logistic Regression\n",
    "\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems. You can construct a logistic regression model using the LogisticRegression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.97%\tSigma: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dominic_Bett\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use 4 nonlinear machine learning algorithms.\n",
    "\n",
    "### k-Nearest Neighbors\n",
    "The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction. You can construct a KNN model using the KNeighborsClassifier class.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.80%\tSigma: 0.12\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function. You can construct a Naive Bayes model using the GaussianNB class.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20.70%\tSigma: 0.37\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees\n",
    "\n",
    "Classification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index). You can construct a CART model using the DecisionTreeClassifier class.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.20%\tSigma: 0.08\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Support Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default. You can construct an SVM model using the SVC class.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 19.73%\tSigma: 0.25\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Machine Learning Algorithms\n",
    "\n",
    "It is important to compare the performance of multiple different machine learning algorithms consistently.\n",
    "Each model has different performance characteristics. Using resampling methods like cross-validation, we can get an estimate for how accurate each model may be on unseen data. We need to be able to use these estimates to choose one or two best models from the suite of models that we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:\tAccuracy: 22.97%\t Sigma: 0.41\n",
      "LDA:\tAccuracy: 22.97%\t Sigma: 0.41\n",
      "KNN:\tAccuracy: 14.80%\t Sigma: 0.12\n",
      "CART:\tAccuracy: 13.64%\t Sigma: 0.09\n",
      "NB:\tAccuracy: 20.70%\t Sigma: 0.37\n",
      "SVM:\tAccuracy: 19.73%\t Sigma: 0.25\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbe9d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "from matplotlib import pyplot\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s:\\tAccuracy: %.2f%%\\t Sigma: %.2f\" % (name, cv_results.mean()*100, cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Algorithms\n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. The three bagging models covered in this section are as follows:\n",
    "* Bagged Decision Trees.\n",
    "* Random Forest.\n",
    "* Extra Trees.\n",
    "\n",
    "### Bagged Decision Trees\n",
    "15.2.1 Bagged Decision Trees Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning. In the example below is an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created.\n",
    "Running the example, we get a robust estimate of model accuracy.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.61%\tSigma: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 50\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. You can construct a Random Forest model for classification using the RandomForestClassifier class. The example below demonstrates using Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "Running the example provides a mean estimate of classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 15.54%\tSigma: 0.11\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "max_features = 2\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees\n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class. The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "Running the example provides a mean estimate of classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.46%\tSigma: 0.12\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence. Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction. The two most common boosting ensemble machine learning algorithms are:\n",
    "* AdaBoost.\n",
    "* Stochastic Gradient Boosting.\n",
    "\n",
    "### AdaBoost\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay less attention to them in the construction of subsequent models. You can construct an AdaBoost model for classification using the AdaBoostClassifier class4 . The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 21.47%\tSigma: 0.26\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles. You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class. The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 21.52%\tSigma: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data. The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from sub-models, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "\n",
    "You can create a voting ensemble model for classification using the VotingClassifier class. The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.40%\tSigma: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "estimators.append(('LR', LogisticRegression()))\n",
    "estimators.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "estimators.append(('KNN', KNeighborsClassifier()))\n",
    "estimators.append(('CART', DecisionTreeClassifier()))\n",
    "estimators.append(('NB', GaussianNB()))\n",
    "estimators.append(('SVM', SVC()))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As a bonus...\n",
    "### Neural network\n",
    "\n",
    "Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13.50%\tSigma: 0.23\n"
     ]
    }
   ],
   "source": [
    "#Neural network Classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print('Accuracy: %.2f%%\\tSigma: %.2f' % (results.mean()*100, results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Preprocessing not needed for this particular dataset\n",
    "* KFold Validation is the preffered method of checking the accuracy of our prediction models\n",
    "* All models performed dismally given the fact that we are predicting against using only two features: Prot1 & Prot2\n",
    "* ML Algorithm categories (Regression algorithms, Classification algorithms) were evaluated\n",
    "* Although Linear algorithms (LR & LDA) outperformed the rest, my choice for the best algorithm for this ML task is ```Stochastic Gradient Boosting```\n",
    "* This algorithm is a Boosting algorithm, meaning it learns from the mistakes of the its models and corrects itself, I think it is the most to make our predictions\n",
    "* There could be arguements to support other perspectives on this as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Prediction\n",
    "Now we need to use Machine Learning to make predictions for Missing PPIs\n",
    "In this case, we will use ```Stochastic Gradient Boosting Classifier``` since it showed great promise\n",
    "You can as well use any prediction model\n",
    "We will fit our chosen model with the actual values and make predictions on Non PPI values\n",
    "Note that the model is just about 22% accurate :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non PPI (Numerized) List:\n",
      "[[32 54]\n",
      " [21 28]\n",
      " [ 4 36]\n",
      " ..., \n",
      " [ 8 80]\n",
      " [28 51]\n",
      " [27 75]]\n",
      "Shape:  (2737L, 2L)\n"
     ]
    }
   ],
   "source": [
    "# Read Non-PPI Dataset (Numerized) on which we predict upon\n",
    "fileName = 'Data/Processed/Numerized_Allergy_and_Asthma_nonPPIs.txt'\n",
    "names = ['Prot1', 'Prot2']\n",
    "nonPPIDataNumerized = read_csv(fileName, delimiter='\\t', names=names)\n",
    "\n",
    "nonPPINumerizedList = nonPPIDataNumerized.values\n",
    "print 'Non PPI (Numerized) List:\\n', nonPPINumerizedList\n",
    "print 'Shape: ', nonPPINumerizedList.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non PPI (Numerized) List:\n",
      "[['RNASE3' 'TPSAB1']\n",
      " ['RETNLB' 'CSF3R']\n",
      " ['PDCD1' 'IL5']\n",
      " ..., \n",
      " ['CCL2' 'CYSLTR1']\n",
      " ['CCL8' 'CSF3R']\n",
      " ['CCR8' 'SIGLEC8']]\n",
      "Shape:  (2737L, 2L)\n"
     ]
    }
   ],
   "source": [
    "# Read the actual Non-PPI Dataset so we can use it to save the reslts\n",
    "fileName = 'Data/Processed/Allergy_and_Asthma_nonPPIs.txt'\n",
    "names = ['Prot1', 'Prot2']\n",
    "nonPPIData = read_csv(fileName, delimiter='\\t', names=names)\n",
    "\n",
    "nonPPIList = nonPPIData.values\n",
    "print 'Non PPI (Numerized) List:\\n', nonPPIList\n",
    "print 'Shape: ', nonPPIList.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted PPI List:\n",
      "[['IL13RA1' 'IL3']\n",
      " ['PDCD1' 'PMCH']\n",
      " ['CCL2' 'ADRB2']\n",
      " ..., \n",
      " ['STAT5A' 'IL25']\n",
      " ['IL1RL1' 'CCL26']\n",
      " ['CCR8' 'SIGLEC8']]\n",
      "Shape:  (287L, 2L)\n"
     ]
    }
   ],
   "source": [
    "# Make the predictions and save the results in file\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "model.fit(X,Y) #fit model with actual PPI values\n",
    "predictions = model.predict(nonPPINumerizedList) #Make predictions on NonPPIs\n",
    "\n",
    "results = []\n",
    "for prediction,  nonPPI in zip(predictions, nonPPIList):\n",
    "    if prediction >= 900:\n",
    "        results.append(nonPPI)\n",
    "        \n",
    "results = np.array(results)\n",
    "\n",
    "print 'Predicted PPI List:\\n', results\n",
    "print 'Shape: ', results.shape\n",
    "\n",
    "fileName = 'Data/Results/PredictionResultsPPIs.txt'\n",
    "#np.savetxt(fileName, results, fmt='%s', sep='\\t')\n",
    "pd.DataFrame(results).to_csv(fileName, sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "As you can see above, we have our results comprising of 287 PPIs. These were selected with the criteria that they be predicted to have PPI scores of 900 or greater, meaning that they  fall in the class of 900 or 950. Whether this is the best approach is debatable since the ML only looks at the scores as classes and not actual weighted values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
